{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split # type: ignore\n",
    "from sklearn.preprocessing import Binarizer, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "#from aif360.algorithms.inprocessing import AdversarialDebiasing\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Classification\n",
    "\n",
    "### 1.1 Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "id": "lAQ0YA7Q2hvc",
    "outputId": "508e7ec1-a327-4d11-c59a-a65d07d23d7e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>workclass_Federal-gov</th>\n",
       "      <th>workclass_Local-gov</th>\n",
       "      <th>workclass_Never-worked</th>\n",
       "      <th>workclass_Private</th>\n",
       "      <th>...</th>\n",
       "      <th>native-country_Puerto-Rico</th>\n",
       "      <th>native-country_Scotland</th>\n",
       "      <th>native-country_South</th>\n",
       "      <th>native-country_Taiwan</th>\n",
       "      <th>native-country_Thailand</th>\n",
       "      <th>native-country_Trinadad&amp;Tobago</th>\n",
       "      <th>native-country_United-States</th>\n",
       "      <th>native-country_Vietnam</th>\n",
       "      <th>native-country_Yugoslavia</th>\n",
       "      <th>income_&gt;50K</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>77516</td>\n",
       "      <td>13</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>83311</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>215646</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>234721</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>338409</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32556</th>\n",
       "      <td>0</td>\n",
       "      <td>257302</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32557</th>\n",
       "      <td>1</td>\n",
       "      <td>154374</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32558</th>\n",
       "      <td>1</td>\n",
       "      <td>151910</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32559</th>\n",
       "      <td>0</td>\n",
       "      <td>201490</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32560</th>\n",
       "      <td>1</td>\n",
       "      <td>287927</td>\n",
       "      <td>9</td>\n",
       "      <td>15024</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32561 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  fnlwgt  education-num  capital-gain  capital-loss  hours-per-week  \\\n",
       "0        1   77516             13          2174             0              40   \n",
       "1        1   83311             13             0             0              13   \n",
       "2        1  215646              9             0             0              40   \n",
       "3        1  234721              7             0             0              40   \n",
       "4        0  338409             13             0             0              40   \n",
       "...    ...     ...            ...           ...           ...             ...   \n",
       "32556    0  257302             12             0             0              38   \n",
       "32557    1  154374              9             0             0              40   \n",
       "32558    1  151910              9             0             0              40   \n",
       "32559    0  201490              9             0             0              20   \n",
       "32560    1  287927              9         15024             0              40   \n",
       "\n",
       "       workclass_Federal-gov  workclass_Local-gov  workclass_Never-worked  \\\n",
       "0                      False                False                   False   \n",
       "1                      False                False                   False   \n",
       "2                      False                False                   False   \n",
       "3                      False                False                   False   \n",
       "4                      False                False                   False   \n",
       "...                      ...                  ...                     ...   \n",
       "32556                  False                False                   False   \n",
       "32557                  False                False                   False   \n",
       "32558                  False                False                   False   \n",
       "32559                  False                False                   False   \n",
       "32560                  False                False                   False   \n",
       "\n",
       "       workclass_Private  ...  native-country_Puerto-Rico  \\\n",
       "0                  False  ...                       False   \n",
       "1                  False  ...                       False   \n",
       "2                   True  ...                       False   \n",
       "3                   True  ...                       False   \n",
       "4                   True  ...                       False   \n",
       "...                  ...  ...                         ...   \n",
       "32556               True  ...                       False   \n",
       "32557               True  ...                       False   \n",
       "32558               True  ...                       False   \n",
       "32559               True  ...                       False   \n",
       "32560              False  ...                       False   \n",
       "\n",
       "       native-country_Scotland  native-country_South  native-country_Taiwan  \\\n",
       "0                        False                 False                  False   \n",
       "1                        False                 False                  False   \n",
       "2                        False                 False                  False   \n",
       "3                        False                 False                  False   \n",
       "4                        False                 False                  False   \n",
       "...                        ...                   ...                    ...   \n",
       "32556                    False                 False                  False   \n",
       "32557                    False                 False                  False   \n",
       "32558                    False                 False                  False   \n",
       "32559                    False                 False                  False   \n",
       "32560                    False                 False                  False   \n",
       "\n",
       "       native-country_Thailand  native-country_Trinadad&Tobago  \\\n",
       "0                        False                           False   \n",
       "1                        False                           False   \n",
       "2                        False                           False   \n",
       "3                        False                           False   \n",
       "4                        False                           False   \n",
       "...                        ...                             ...   \n",
       "32556                    False                           False   \n",
       "32557                    False                           False   \n",
       "32558                    False                           False   \n",
       "32559                    False                           False   \n",
       "32560                    False                           False   \n",
       "\n",
       "       native-country_United-States  native-country_Vietnam  \\\n",
       "0                              True                   False   \n",
       "1                              True                   False   \n",
       "2                              True                   False   \n",
       "3                              True                   False   \n",
       "4                             False                   False   \n",
       "...                             ...                     ...   \n",
       "32556                          True                   False   \n",
       "32557                          True                   False   \n",
       "32558                          True                   False   \n",
       "32559                          True                   False   \n",
       "32560                          True                   False   \n",
       "\n",
       "       native-country_Yugoslavia  income_>50K  \n",
       "0                          False        False  \n",
       "1                          False        False  \n",
       "2                          False        False  \n",
       "3                          False        False  \n",
       "4                          False        False  \n",
       "...                          ...          ...  \n",
       "32556                      False        False  \n",
       "32557                      False         True  \n",
       "32558                      False        False  \n",
       "32559                      False        False  \n",
       "32560                      False         True  \n",
       "\n",
       "[32561 rows x 101 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "columns = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\",\n",
    "           \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\",\n",
    "           \"hours-per-week\", \"native-country\", \"income\"]\n",
    "data = pd.read_csv(url, header=None, names=columns, na_values=\" ?\", skipinitialspace=True)\n",
    "\n",
    "# Drop rows with missing values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Need to copy the data as we don't want a binarized part for testing the privacy\n",
    "data_copy = data.copy()\n",
    "\n",
    "# Binarize the 'age' attribute\n",
    "binarizer = Binarizer(threshold=30)\n",
    "data['age'] = binarizer.fit_transform(data[['age']])\n",
    "\n",
    "# Convert categorical variables to dummy variables\n",
    "data = pd.get_dummies(data, drop_first=True)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amZsndYi2hvd"
   },
   "source": [
    "### 1.2 Train a logistic regression classifier and measure its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sx9P9epS2hve",
    "outputId": "22ca08a7-8b73-4e7b-8fc8-d6b0eea4d888"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Metrics: {'Accuracy': 0.8552712384851586, 'Precision': 0.7271750805585392, 'Recall': 0.5991150442477876, 'F1 Score': 0.6569626394953906}\n"
     ]
    }
   ],
   "source": [
    "# Split the data into features and target variable\n",
    "X = data.drop('income_>50K', axis=1)\n",
    "y = data['income_>50K']\n",
    "\n",
    "# Split the data into train, validation, and test sets (70% train, 15% validation, 15% test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train a logistic regression classifier\n",
    "classifier = LogisticRegression(max_iter=2000)\n",
    "classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = classifier.predict(X_test_scaled)\n",
    "\n",
    "# Measure performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "performance_metrics = {\n",
    "    'Accuracy': accuracy,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1 Score': f1\n",
    "}\n",
    "\n",
    "print(\"Performance Metrics:\", performance_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VheTnJs52hvf"
   },
   "source": [
    "# 2 - Fairness\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkseqmx15568"
   },
   "source": [
    "**2.2 Fairness Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cF2DXG2v6B8y",
    "outputId": "808d6ab7-607d-4eb9-b68a-aeae013aa7aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fairness Metrics (Original Classifier):\n",
      "Age - Demographic Parity Difference: 0.2461163789376603, Demographic Parity Ratio: 0.08738725259693071\n",
      "Sex - Demographic Parity Difference: 0.1725282306338493, Demographic Parity Ratio: 0.3045489515167733\n"
     ]
    }
   ],
   "source": [
    "from fairlearn.metrics import demographic_parity_difference, demographic_parity_ratio\n",
    "\n",
    "# Define sensitive features: 'age' and 'sex_Male'\n",
    "age_sensitive = X_test['age']\n",
    "sex_sensitive = X_test['sex_Male']\n",
    "\n",
    "# Compute fairness metrics for the original classifier\n",
    "dpd_age = demographic_parity_difference(y_test, y_pred, sensitive_features=age_sensitive)\n",
    "dpr_age = demographic_parity_ratio(y_test, y_pred, sensitive_features=age_sensitive)\n",
    "\n",
    "dpd_sex = demographic_parity_difference(y_test, y_pred, sensitive_features=sex_sensitive)\n",
    "dpr_sex = demographic_parity_ratio(y_test, y_pred, sensitive_features=sex_sensitive)\n",
    "\n",
    "print(\"Fairness Metrics (Original Classifier):\")\n",
    "print(f\"Age - Demographic Parity Difference: {dpd_age}, Demographic Parity Ratio: {dpr_age}\")\n",
    "print(f\"Sex - Demographic Parity Difference: {dpd_sex}, Demographic Parity Ratio: {dpr_sex}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pZcQmqo63X4"
   },
   "source": [
    "**2.3 Apply a Fairness Mitigation Technique**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rs2BFOLi64KU",
    "outputId": "0d988328-5692-484c-cdb4-a8a358cc6f8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reweighted Classifier Performance Metrics:\n",
      "Accuracy: 0.8380757420675538\n",
      "Precision: 0.6968641114982579\n",
      "Recall: 0.5309734513274337\n",
      "F1 Score: 0.6027122049221497\n"
     ]
    }
   ],
   "source": [
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Convert data into BinaryLabelDataset format\n",
    "binary_data = BinaryLabelDataset(\n",
    "    df=pd.concat([X_train, y_train], axis=1),  # Combine features and target\n",
    "    label_names=['income_>50K'],               # Target column\n",
    "    protected_attribute_names=['age', 'sex_Male']  # Sensitive attributes\n",
    ")\n",
    "\n",
    "# Apply reweighting to the data\n",
    "rw = Reweighing(unprivileged_groups=[{'age': 0}], privileged_groups=[{'age': 1}])  # Reweigh based on age\n",
    "rw.fit(binary_data)  # Fit the reweighing model\n",
    "reweighted_data = rw.transform(binary_data)  # Apply reweighting\n",
    "\n",
    "# Train the classifier using the reweighted data\n",
    "classifier_reweighted = LogisticRegression(max_iter=2000)\n",
    "classifier_reweighted.fit(X_train_scaled, y_train, sample_weight=reweighted_data.instance_weights)\n",
    "\n",
    "# Predict and measure performance on the reweighted classifier\n",
    "y_pred_reweighted = classifier_reweighted.predict(X_test_scaled)\n",
    "\n",
    "# Performance metrics for reweighted classifier\n",
    "accuracy_reweighted = accuracy_score(y_test, y_pred_reweighted)\n",
    "precision_reweighted = precision_score(y_test, y_pred_reweighted)\n",
    "recall_reweighted = recall_score(y_test, y_pred_reweighted)\n",
    "f1_reweighted = f1_score(y_test, y_pred_reweighted)\n",
    "\n",
    "print(\"\\nReweighted Classifier Performance Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_reweighted}\")\n",
    "print(f\"Precision: {precision_reweighted}\")\n",
    "print(f\"Recall: {recall_reweighted}\")\n",
    "print(f\"F1 Score: {f1_reweighted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHd4fndl7B3p"
   },
   "source": [
    "**2.4 Report Fairness Metrics on the Classifier and the Fair Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpd_reweighted_age = demographic_parity_difference(y_test, y_pred_reweighted, sensitive_features=age_sensitive)\n",
    "dpr_reweighted_age = demographic_parity_ratio(y_test, y_pred_reweighted, sensitive_features=age_sensitive)\n",
    "\n",
    "dpd_reweighted_sex = demographic_parity_difference(y_test, y_pred_reweighted, sensitive_features=sex_sensitive)\n",
    "dpr_reweighted_sex = demographic_parity_ratio(y_test, y_pred_reweighted, sensitive_features=sex_sensitive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gr9YTN9G7DWl",
    "outputId": "6855a3f5-399a-4eb6-8d49-a64e205a618f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fairness Metrics Comparison (Original vs Reweighted Classifier):\n",
      "                                     Original Classifier  \\\n",
      "Metric                                                     \n",
      "Demographic Parity Difference (Age)             0.246116   \n",
      "Demographic Parity Ratio (Age)                  0.087387   \n",
      "Demographic Parity Difference (Sex)             0.172528   \n",
      "Demographic Parity Ratio (Sex)                  0.304549   \n",
      "\n",
      "                                     Reweighted Classifier  \n",
      "Metric                                                      \n",
      "Demographic Parity Difference (Age)               0.100166  \n",
      "Demographic Parity Ratio (Age)                    0.519463  \n",
      "Demographic Parity Difference (Sex)               0.140902  \n",
      "Demographic Parity Ratio (Sex)                    0.368751  \n",
      "\n",
      "Performance Metrics Comparison (Original vs Reweighted Classifier):\n",
      "           Original Classifier  Reweighted Classifier\n",
      "Metric                                               \n",
      "Accuracy              0.855271               0.838076\n",
      "Precision             0.727175               0.696864\n",
      "Recall                0.599115               0.530973\n",
      "F1 Score              0.656963               0.602712\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Fairness metrics for the original classifier\n",
    "metrics_original = {\n",
    "    'Metric': ['Demographic Parity Difference (Age)', 'Demographic Parity Ratio (Age)',\n",
    "               'Demographic Parity Difference (Sex)', 'Demographic Parity Ratio (Sex)'],\n",
    "    'Original Classifier': [dpd_age, dpr_age, dpd_sex, dpr_sex]\n",
    "}\n",
    "\n",
    "# Performance metrics for the original classifier\n",
    "metrics_original_performance = {\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "    'Original Classifier': [accuracy, precision, recall, f1]\n",
    "}\n",
    "\n",
    "# Fairness metrics for the reweighted classifier\n",
    "metrics_reweighted = {\n",
    "    'Metric': ['Demographic Parity Difference (Age)', 'Demographic Parity Ratio (Age)',\n",
    "               'Demographic Parity Difference (Sex)', 'Demographic Parity Ratio (Sex)'],\n",
    "    'Reweighted Classifier': [dpd_reweighted_age, dpr_reweighted_age, dpd_reweighted_sex, dpr_reweighted_sex]\n",
    "}\n",
    "\n",
    "# Performance metrics for the reweighted classifier\n",
    "metrics_reweighted_performance = {\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "    'Reweighted Classifier': [accuracy_reweighted, precision_reweighted, recall_reweighted, f1_reweighted]\n",
    "}\n",
    "\n",
    "# Convert dictionaries to dataframes\n",
    "df_original = pd.DataFrame(metrics_original)\n",
    "df_original_performance = pd.DataFrame(metrics_original_performance)\n",
    "df_reweighted = pd.DataFrame(metrics_reweighted)\n",
    "df_reweighted_performance = pd.DataFrame(metrics_reweighted_performance)\n",
    "\n",
    "# Display fairness metrics and performance metrics in tables\n",
    "print(\"Fairness Metrics Comparison (Original vs Reweighted Classifier):\")\n",
    "print(pd.concat([df_original.set_index('Metric'), df_reweighted.set_index('Metric')], axis=1))\n",
    "\n",
    "print(\"\\nPerformance Metrics Comparison (Original vs Reweighted Classifier):\")\n",
    "print(pd.concat([df_original_performance.set_index('Metric'), df_reweighted_performance.set_index('Metric')], axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AoHxSU_b2hvf"
   },
   "source": [
    "# 3 - Privacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5GHWrOya2hvf"
   },
   "source": [
    "### 3.1 First cross tabulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the distribution is the following: \n",
      " sex  Female  Male\n",
      "age              \n",
      "17      186   209\n",
      "18      268   282\n",
      "19      356   356\n",
      "20      363   390\n",
      "21      329   391\n",
      "..      ...   ...\n",
      "85        1     2\n",
      "86        1     0\n",
      "87        0     1\n",
      "88        1     2\n",
      "90       14    29\n",
      "\n",
      "[73 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Cross-tabulation on age and sex\n",
    "sensitive_crosstab = pd.crosstab(data_copy['age'], data_copy['sex'])\n",
    "\n",
    "print(\"the distribution is the following: \\n\", sensitive_crosstab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Local differential privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Original Age  Private Age Original Sex Private Sex\n",
      "0                39           39         Male        Male\n",
      "1                50           50         Male        Male\n",
      "2                38           38         Male        Male\n",
      "3                53           53         Male        Male\n",
      "4                28           28       Female      Female\n",
      "...             ...          ...          ...         ...\n",
      "32556            27           27       Female        Male\n",
      "32557            40           40         Male        Male\n",
      "32558            58           57       Female        Male\n",
      "32559            22           22         Male        Male\n",
      "32560            52           51       Female      Female\n",
      "\n",
      "[32561 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# making a private dataset\n",
    "private_data = data_copy.copy()\n",
    "\n",
    "#We can change the values of the epsilon to make tests on them here, the current ones are the ones\n",
    "#that seemed the more coherent after testing multiple ones\n",
    "epsilon_age = 0.9\n",
    "epsilon_sex = 0.2 #so 20% of the sex values are changed\n",
    "\n",
    "#Apply Laplace noise and round it(for age)\n",
    "def laplace_noise(value, epsilon):\n",
    "    return round(value + np.random.laplace(0, 1 / epsilon))\n",
    "\n",
    "#Apply randomized response (for sex)\n",
    "def randomized_response(value, epsilon):\n",
    "    if np.random.rand() < epsilon_sex: #this way, the valu of epsilo_sex is the proportion of changed sex\n",
    "        return 'Male' if (value == 'Female') else 'Female'\n",
    "    return value\n",
    "\n",
    "#remark: we can't take a epsilon too small for sex, else we risk blurring information on sex inequalities\n",
    "\n",
    "#Apply local differential privacy to age and sex on dataPrivate\n",
    "private_data['age'] = data_copy['age'].apply(lambda x: laplace_noise(x, epsilon_age))\n",
    "private_data['sex'] = data_copy['sex'].apply(lambda x: randomized_response(x, epsilon_sex))\n",
    "\n",
    "# Concatenate the 'age' and 'sex' columns from both original (dataCopy) and private (dataPrivate) data\n",
    "comparison_df = pd.concat([data_copy[['age', 'sex']], private_data[['age', 'sex']]], axis=1)\n",
    "comparison_df.columns = ['Original Age', 'Original Sex', 'Private Age', 'Private Sex']\n",
    "comparison_df = comparison_df[['Original Age', 'Private Age', 'Original Sex', 'Private Sex']]\n",
    "\n",
    "# Show the comparison to see how the private dataset differs from the original one\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Cross tabulation on the private dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the distribution on the private data is the following:\n",
      " sex  Female  Male\n",
      "age              \n",
      "16        5    11\n",
      "17      183   204\n",
      "18      282   253\n",
      "19      341   370\n",
      "20      368   382\n",
      "..      ...   ...\n",
      "86        1     0\n",
      "88        2     2\n",
      "89        1     0\n",
      "90       19    22\n",
      "91        0     1\n",
      "\n",
      "[75 rows x 2 columns]\n",
      "Estimation error:\n",
      " sex  Female  Male\n",
      "age              \n",
      "16       -5   -11\n",
      "17        3     5\n",
      "18      -14    29\n",
      "19       15   -14\n",
      "20       -5     8\n",
      "..      ...   ...\n",
      "87        0     1\n",
      "88       -1     0\n",
      "89       -1     0\n",
      "90       -5     7\n",
      "91        0    -1\n",
      "\n",
      "[76 rows x 2 columns]\n",
      "note: negative values mean there are more in the private dataset than in the original one\n"
     ]
    }
   ],
   "source": [
    "#Cross-tabulation for the private data\n",
    "private_crosstab = pd.crosstab(private_data['age'], private_data['sex'])\n",
    "\n",
    "print(\"the distribution on the private data is the following:\\n\", private_crosstab)\n",
    "\n",
    "# Align both crosstabs, necessary, else it returns an empty dataset\n",
    "sensitive_crosstab, private_crosstab = sensitive_crosstab.align(private_crosstab, join='outer', axis=0, fill_value=0)\n",
    "\n",
    "#Calculate the estimation errors\n",
    "#this shows the differences in distribution between the private and original dataset\n",
    "comparison_crosstab = sensitive_crosstab.subtract(private_crosstab, fill_value=0)\n",
    "print(\"Estimation error:\\n\", comparison_crosstab)\n",
    "\n",
    "print(\"note: negative values mean there are more in the private dataset than in the original one\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Data splitting and classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we binarize the ages again\n",
    "private_data['age'] = binarizer.fit_transform(private_data[['age']])\n",
    "\n",
    "# Convert categorical variables to dummy variables\n",
    "private_data = pd.get_dummies(private_data, drop_first=True)\n",
    "\n",
    "#now we make a process similar to the one from (1):\n",
    "\n",
    "# Split the data into features and target variable\n",
    "X_p = private_data.drop('income_>50K', axis=1)\n",
    "Y_p = private_data['income_>50K']\n",
    "\n",
    "# Split the data into train, validation, and test sets (70% train, 15% validation, 15% test)\n",
    "X_train_p, X_temp_p, Y_train_p, Y_temp_p = train_test_split(X_p, Y_p, test_size=0.3, random_state=42)\n",
    "X_val_p, X_test_p, Y_val_p, Y_test_p = train_test_split(X_temp_p, Y_temp_p, test_size=0.5, random_state=42)\n",
    "\n",
    "# Scale the data\n",
    "#scaler was already defined in (1)\n",
    "X_train_scaled_p = scaler.fit_transform(X_train_p)\n",
    "X_val_scaled_p = scaler.transform(X_val_p)\n",
    "X_test_scaled_p = scaler.transform(X_test_p)\n",
    "\n",
    "# Train a logistic regression classifier\n",
    "private_classifier = LogisticRegression(max_iter=2000)\n",
    "private_classifier.fit(X_train_scaled_p, Y_train_p)\n",
    "\n",
    "\n",
    "# Predict on the test set\n",
    "Y_pred_p = private_classifier.predict(X_test_scaled_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled_p.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Performances measuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Metrics of the original dataset: {'Accuracy': 0.8552712384851586, 'Precision': 0.7271750805585392, 'Recall': 0.5991150442477876, 'F1 Score': 0.6569626394953906}\n",
      "Performance Metrics of the private dataset:  {'Accuracy': 0.8528147389969294, 'Precision': 0.7236126224156693, 'Recall': 0.588495575221239, 'F1 Score': 0.6490971205466081}\n"
     ]
    }
   ],
   "source": [
    "# Measure performance on the private classifier\n",
    "private_accuracy = accuracy_score(Y_test_p, Y_pred_p)\n",
    "private_precision = precision_score(Y_test_p, Y_pred_p)\n",
    "private_recall = recall_score(Y_test_p, Y_pred_p)\n",
    "private_f1 = f1_score(Y_test_p, Y_pred_p)\n",
    "\n",
    "private_performance_metrics = {\n",
    "    'Accuracy': private_accuracy,\n",
    "    'Precision': private_precision,\n",
    "    'Recall': private_recall,\n",
    "    'F1 Score': private_f1\n",
    "}\n",
    "\n",
    "print(\"Performance Metrics of the original dataset:\", performance_metrics)\n",
    "print(\"Performance Metrics of the private dataset: \", private_performance_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Privacy and Fairness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Private+Fair Classifier Performance Metrics:\n",
      "Accuracy: 0.8397134083930399\n",
      "Precision: 0.7019790454016298\n",
      "Recall: 0.5336283185840708\n",
      "F1 Score: 0.6063348416289592\n"
     ]
    }
   ],
   "source": [
    "# Convert the private data into BinaryLabelDataset format\n",
    "binary_private_data = BinaryLabelDataset(\n",
    "    df=pd.concat([X_train_p, Y_train_p], axis=1),\n",
    "    label_names=['income_>50K'],\n",
    "    protected_attribute_names=['age']\n",
    ")\n",
    "\n",
    "# Apply Reweighing to the private data\n",
    "rw_private = Reweighing(unprivileged_groups=[{'age': 0}], privileged_groups=[{'age': 1}])\n",
    "rw_private.fit(binary_private_data)  # Fit the Reweighing algorithm\n",
    "reweighted_private_data = rw_private.transform(binary_private_data)  # Apply weights\n",
    "\n",
    "# Train the logistic regression classifier with reweighted data\n",
    "private_fair_classifier = LogisticRegression(max_iter=2000)\n",
    "private_fair_classifier.fit(X_train_scaled_p, Y_train_p, sample_weight=reweighted_private_data.instance_weights)\n",
    "\n",
    "# Predict on the test set\n",
    "Y_pred_private_fair = private_fair_classifier.predict(X_test_scaled_p)\n",
    "\n",
    "# Performance metrics for Private+Fair Classifier\n",
    "accuracy_private_fair = accuracy_score(Y_test_p, Y_pred_private_fair)\n",
    "precision_private_fair = precision_score(Y_test_p, Y_pred_private_fair)\n",
    "recall_private_fair = recall_score(Y_test_p, Y_pred_private_fair)\n",
    "f1_private_fair = f1_score(Y_test_p, Y_pred_private_fair)\n",
    "\n",
    "# Display metrics\n",
    "print(\"Private+Fair Classifier Performance Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_private_fair}\")\n",
    "print(f\"Precision: {precision_private_fair}\")\n",
    "print(f\"Recall: {recall_private_fair}\")\n",
    "print(f\"F1 Score: {f1_private_fair}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Private+Fair Classifier Fairness Metrics:\n",
      "Age - Demographic Parity Difference: 0.10170206196731359, Demographic Parity Ratio: 0.5123893903072215\n",
      "Sex - Demographic Parity Difference: 0.08781984139898075, Demographic Parity Ratio: 0.5851516969208279\n"
     ]
    }
   ],
   "source": [
    "age_sensitive_private = X_test_p['age']  \n",
    "sex_sensitive_private = X_test_p['sex_Male'] \n",
    "\n",
    "# Fairness metrics for the Private+Fair Classifier\n",
    "dpd_private_fair_age = demographic_parity_difference(Y_test_p, Y_pred_private_fair, sensitive_features=age_sensitive_private)\n",
    "dpr_private_fair_age = demographic_parity_ratio(Y_test_p, Y_pred_private_fair, sensitive_features=age_sensitive_private)\n",
    "\n",
    "dpd_private_fair_sex = demographic_parity_difference(Y_test_p, Y_pred_private_fair, sensitive_features=sex_sensitive_private)\n",
    "dpr_private_fair_sex = demographic_parity_ratio(Y_test_p, Y_pred_private_fair, sensitive_features=sex_sensitive_private)\n",
    "\n",
    "print(\"\\nPrivate+Fair Classifier Fairness Metrics:\")\n",
    "print(f\"Age - Demographic Parity Difference: {dpd_private_fair_age}, Demographic Parity Ratio: {dpr_private_fair_age}\")\n",
    "print(f\"Sex - Demographic Parity Difference: {dpd_private_fair_sex}, Demographic Parity Ratio: {dpr_private_fair_sex}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fairness Metrics Comparison (Original vs Reweighted vs Private+Fair Classifier):\n",
      "                                     Original Classifier  \\\n",
      "Metric                                                     \n",
      "Demographic Parity Difference (Age)             0.246116   \n",
      "Demographic Parity Ratio (Age)                  0.087387   \n",
      "Demographic Parity Difference (Sex)             0.172528   \n",
      "Demographic Parity Ratio (Sex)                  0.304549   \n",
      "\n",
      "                                     Reweighted Classifier  \\\n",
      "Metric                                                       \n",
      "Demographic Parity Difference (Age)               0.100166   \n",
      "Demographic Parity Ratio (Age)                    0.519463   \n",
      "Demographic Parity Difference (Sex)               0.140902   \n",
      "Demographic Parity Ratio (Sex)                    0.368751   \n",
      "\n",
      "                                     Private+Fair Classifier  \n",
      "Metric                                                        \n",
      "Demographic Parity Difference (Age)                 0.101702  \n",
      "Demographic Parity Ratio (Age)                      0.512389  \n",
      "Demographic Parity Difference (Sex)                 0.087820  \n",
      "Demographic Parity Ratio (Sex)                      0.585152  \n",
      "\n",
      "Performance Metrics Comparison (Original vs Reweighted vs Private+Fair Classifier):\n",
      "           Original Classifier  Reweighted Classifier  Private+Fair Classifier\n",
      "Metric                                                                        \n",
      "Accuracy              0.855271               0.838076                 0.839713\n",
      "Precision             0.727175               0.696864                 0.701979\n",
      "Recall                0.599115               0.530973                 0.533628\n",
      "F1 Score              0.656963               0.602712                 0.606335\n"
     ]
    }
   ],
   "source": [
    "# Fairness metrics for private+fair classifier\n",
    "metrics_private_fair = {\n",
    "    'Metric': ['Demographic Parity Difference (Age)', 'Demographic Parity Ratio (Age)',\n",
    "               'Demographic Parity Difference (Sex)', 'Demographic Parity Ratio (Sex)'],\n",
    "    'Private+Fair Classifier': [dpd_private_fair_age, dpr_private_fair_age, dpd_private_fair_sex, dpr_private_fair_sex]\n",
    "}\n",
    "\n",
    "# Performance metrics for private+fair classifier\n",
    "metrics_private_fair_performance = {\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "    'Private+Fair Classifier': [accuracy_private_fair, precision_private_fair, recall_private_fair, f1_private_fair]\n",
    "}\n",
    "\n",
    "# Convert to DataFrames\n",
    "df_private_fair = pd.DataFrame(metrics_private_fair)\n",
    "df_private_fair_performance = pd.DataFrame(metrics_private_fair_performance)\n",
    "\n",
    "# Display comparison tables\n",
    "print(\"\\nFairness Metrics Comparison (Original vs Reweighted vs Private+Fair Classifier):\")\n",
    "print(pd.concat([df_original.set_index('Metric'), \n",
    "                 df_reweighted.set_index('Metric'), \n",
    "                 df_private_fair.set_index('Metric')], axis=1))\n",
    "\n",
    "print(\"\\nPerformance Metrics Comparison (Original vs Reweighted vs Private+Fair Classifier):\")\n",
    "print(pd.concat([df_original_performance.set_index('Metric'), \n",
    "                 df_reweighted_performance.set_index('Metric'), \n",
    "                 df_private_fair_performance.set_index('Metric')], axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "#### Fairness Achieved with Reweighting and Privacy\n",
    " - Both the Fair Classifier and Private+Fair Classifier significantly\n",
    "   reduce demographic disparities compared to the Original Classifier,\n",
    "   achieving much better fairness with respect to Age and Sex.\n",
    "  - Privacy constraints in the Private+Fair Classifier do not compromise\n",
    "   fairness or performance, achieving parity with the Fair Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7c93bKmD2hvg"
   },
   "source": [
    "# 5 - Explainability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'omnixai'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [51], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      5\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdependencies\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01momnixai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtabular\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tabular\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01momnixai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtabular\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TabularTransform\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01momnixai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexplainers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtabular\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TabularExplainer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'omnixai'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"dependencies\")\n",
    "\n",
    "from omnixai.data.tabular import Tabular\n",
    "from omnixai.preprocessing.tabular import TabularTransform\n",
    "from omnixai.explainers.tabular import TabularExplainer\n",
    "import xgboost as xgboost\n",
    "from itertools import cycle, islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Test: {private_accuracy=:.4f}')\n",
    "#confusion_matrix for the private classifier\n",
    "cm = confusion_matrix(Y_test_p, Y_pred_p)\n",
    "\n",
    "TN = cm[0][0]\n",
    "FN = cm[1][0]\n",
    "TP = cm[1][1]\n",
    "FP = cm[0][1]\n",
    "print(f\"Test: {TP=}, {TN=}, {FP=}, {FN=}\")\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, )\n",
    "disp.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Instances where the model is wrong but highly confident:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find instances where the predicted label is different from the actual label\n",
    "miss_indices = np.where(Y_pred_p != Y_test_p)[0]\n",
    "\n",
    "# Create the prediction probabilities\n",
    "proba = private_classifier.predict_proba(X_test_scaled_p)\n",
    "\n",
    "# Find instances where the model is very confident but wrong\n",
    "miss_but_confident = []\n",
    "\n",
    "high_confidence_threshold = 0.95 # the value describing what is considered as a high confidence.\n",
    "\n",
    "for idx in miss_indices:\n",
    "    if max(proba[idx]) > high_confidence_threshold:\n",
    "        miss_but_confident.append(idx)\n",
    "    \n",
    "print(f\"There are {len(miss_but_confident)} instances where the model is very confident but wrong.\")\n",
    "\n",
    "max_nb_instances_to_detail = 2 # number of \"miss_but_confident\" to show in detail , shows all if -1 or none\n",
    "print(f\"Maximum number of instances to analyse is set to {max_nb_instances_to_detail}\")\n",
    "\n",
    "if max_nb_instances_to_detail is not None and max_nb_instances_to_detail >= 0:\n",
    "    slice_nb = min(max_nb_instances_to_detail,len(miss_but_confident))\n",
    "    miss_but_confident_detail = miss_but_confident[:slice_nb]\n",
    "else:\n",
    "    miss_but_confident_detail = miss_but_confident\n",
    "\n",
    "for instance_id in miss_but_confident_detail:\n",
    "    print(f\"\\t-> Instance {instance_id} has label '{Y_test_p.values[instance_id]}' and prediction '{Y_pred_p[instance_id]}', with probs {private_classifier.predict_proba(X_test_scaled_p[instance_id:instance_id+1])[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Explanation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the input data into a pandas dataframe\n",
    "tab_prep_data_df = pd.DataFrame(X_train_scaled_p,columns=X_p.columns)\n",
    "y_col_name = Y_p.name #\"y\"\n",
    "tab_prep_data_df[y_col_name] = Y_train_p.values\n",
    "\n",
    "tab_prep_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the input data in omnixai Tabular form\n",
    "tabular_data = Tabular(\n",
    "   tab_prep_data_df,\n",
    "   target_column=y_col_name\n",
    ")\n",
    "transformer = TabularTransform().fit(tabular_data)\n",
    "class_names = transformer.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for instance_id in miss_but_confident_detail:\n",
    "\n",
    "    print(f\"========== INSTANCE {instance_id}: ==========\")\n",
    "    \n",
    "    explainers = TabularExplainer(\n",
    "      explainers=['lime', 'mace'],                       # The explainers to apply\n",
    "      mode=\"classification\",                             # The task type\n",
    "      data=transformer.invert(X_train_scaled_p),         # The data for initializing the explainers\n",
    "      model=private_classifier,                          # The ML model to explain\n",
    "      preprocess=lambda z: transformer.transform(z),     # Converts raw features into the model inputs\n",
    "      params={\n",
    "            \"mace\": {\"ignored_features\": [\"Age\", \"Sex\"]}# params which cannot change when creating counterfactuals\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    test_instances = transformer.invert(X_test_scaled_p)[instance_id:instance_id+1]\n",
    "    local_explanations = explainers.explain(X=test_instances)\n",
    "\n",
    "    print(\"LIME explanation\")\n",
    "\n",
    "    print(f\"Instance {instance_id} has label {Y_test_p.values[instance_id]} but prediction {Y_pred_private_fair[instance_id]}, with probs {private_classifier.predict_proba(X_test_scaled_p[instance_id:instance_id+1])[0]}\")\n",
    "    local_explanations[\"lime\"].ipython_plot(index=0, class_names=class_names)\n",
    "\n",
    "    print(\"MACE explanation\")\n",
    "\n",
    "    query_df = local_explanations['mace'].get_explanations()[0]['query'].reset_index(drop=True)\n",
    "    query_df.index = [f\"{instance_id}\"] * len(query_df)\n",
    "    \n",
    "    counter_df = local_explanations['mace'].get_explanations()[0]['counterfactual'].reset_index(drop=True)\n",
    "    counter_df.index = [ f\"CF[{cnt}] for {instance_id}\" for cnt in range(len(counter_df))]\n",
    "    \n",
    "    combined_df = pd.concat([query_df, counter_df])\n",
    "\n",
    "    # Highlighting function\n",
    "    def highlight_changes(row):\n",
    "        instance_row = combined_df.iloc[0]  # Reference row for the instance\n",
    "        return [\"color: red\" if row[col] != instance_row[col] else \"\" for col in combined_df.columns]\n",
    "    \n",
    "    styled_df = combined_df.style.apply(highlight_changes, axis=1)\n",
    "    \n",
    "    display(styled_df)\n",
    "    print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Are the noisy values for the sensitive values of Age and Sex attributes responsible for the model being confident and wrong?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at all the test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# regroup data of age and sex from the original and private parts\n",
    "expl_df = comparison_df.copy()\n",
    "\n",
    "expl_test_df = expl_df.loc[X_test_p.index]\n",
    "expl_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expl_test_df[\"real_y\"] = Y_test_p.values\n",
    "expl_test_df[\"pred_y\"] = Y_pred_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expl_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expl_test_df[\"age_abs_diff\"] = abs(expl_test_df[\"Original Age\"]-expl_test_df[\"Private Age\"])\n",
    "expl_test_df[\"sex_change\"] = (expl_test_df[\"Original Sex\"] != expl_test_df[\"Private Sex\"])\n",
    "expl_test_df[\"pred_change\"] = (expl_test_df[\"real_y\"] != expl_test_df[\"pred_y\"])\n",
    "\n",
    "expl_miss_but_conf_df = expl_test_df.iloc[miss_but_confident]\n",
    "\n",
    "expl_test_df[\"new_id\"] = range(len(expl_test_df))\n",
    "expl_miss_but_conf_df[\"new_id\"] = range(len(expl_miss_but_conf_df))\n",
    "\n",
    "check_all_test = True # allows to focus at only \"confident+miss\" or \"all testset\" in the plots\n",
    "\n",
    "expl_check_df = expl_test_df if check_all_test else expl_miss_but_conf_df\n",
    "expl_check_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create smaller dataframes of all combinations of having the sex and/or the prediction\n",
    "snp_df = expl_check_df.loc[expl_check_df[\"sex_change\"] & ~expl_check_df[\"pred_change\"]] #light green\n",
    "nsnp_df = expl_check_df.loc[~expl_check_df[\"sex_change\"] & ~expl_check_df[\"pred_change\"]] #dark green\n",
    "sp_df = expl_check_df.loc[expl_check_df[\"sex_change\"] & expl_check_df[\"pred_change\"]] #light red\n",
    "nsp_df = expl_check_df.loc[~expl_check_df[\"sex_change\"] & expl_check_df[\"pred_change\"]] #dark red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_snp = len(snp_df)\n",
    "nb_nsnp = len(nsnp_df)\n",
    "nb_sp = len(sp_df)\n",
    "nb_nsp = len(nsp_df)\n",
    "print(\"sex change but no pred change:\",nb_snp)\n",
    "print(\"no sex change and no pred change:\",nb_nsnp)\n",
    "print(\"sex change and pred change:\",nb_sp)\n",
    "print(\"no sex change but pred change:\",nb_nsp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_df(df,group_num):\n",
    "    new_df = df.copy()[[\"new_id\",\"age_abs_diff\"]]\n",
    "    new_df[\"group\"] = group_num\n",
    "    return new_df\n",
    "\n",
    "snp_name = \"sex change, no pred change\"\n",
    "nsnp_name = \"no sex change, no pred change\"\n",
    "sp_name = \"sex change, pred change\"\n",
    "nsp_name = \"no sex change, pred change\"\n",
    "\n",
    "regoup_df = prep_df(snp_df,snp_name)\n",
    "regoup_df = pd.concat([regoup_df, prep_df(nsnp_df,nsnp_name)])\n",
    "regoup_df = pd.concat([regoup_df, prep_df(sp_df,sp_name)])\n",
    "regoup_df = pd.concat([regoup_df, prep_df(nsp_df,nsp_name)])\n",
    "\n",
    "regoup_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.pandas\n",
    "\n",
    "sex_pred_color_dict = {snp_name:\"lime\",\n",
    "                       nsnp_name:\"green\",\n",
    "                       sp_name:\"orange\",\n",
    "                       nsp_name:\"red\"}\n",
    "\n",
    "group_order = list(regoup_df[\"group\"].drop_duplicates()) # to enforce name-color association\n",
    "\n",
    "regoup_df.hvplot(\n",
    "    x=\"new_id\", \n",
    "    xlabel=\"\",\n",
    "    y=\"age_abs_diff\",\n",
    "    ylabel=\"absolute age difference\", \n",
    "    kind='scatter', \n",
    "    by=\"group\",\n",
    "    color=[sex_pred_color_dict[name] for name in group_order],\n",
    "    title=\"sex and age change impact on prediction\",\n",
    "    width=1100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create smaller dataframes for having or not a prediction change\n",
    "np_df = expl_check_df.loc[~expl_check_df[\"pred_change\"]] #green\n",
    "p_df = expl_check_df.loc[expl_check_df[\"pred_change\"]] #red\n",
    "\n",
    "regoup_df = prep_df(np_df,\"no pred change\")\n",
    "regoup_df = pd.concat([regoup_df, prep_df(p_df,\"pred change\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_color_dict = {\"no pred change\":\"green\",\n",
    "                   \"pred change\":\"red\"}\n",
    "\n",
    "group_order = list(regoup_df[\"group\"].drop_duplicates()) # to enforce name-color association\n",
    "\n",
    "regoup_df.hvplot(\n",
    "    x=\"new_id\", \n",
    "    xlabel=\"\",\n",
    "    y=\"age_abs_diff\",\n",
    "    ylabel=\"absolute age difference\",\n",
    "    kind='scatter', \n",
    "    by=\"group\",\n",
    "    color=[pred_color_dict[name] for name in group_order],\n",
    "    title=\"age change impact on prediction\",\n",
    "    width=1100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ratio(val1,val2): return round(val1*100/(val1+val2),2)\n",
    "\n",
    "bar_dict = {\"pred changes\":[],\"no pred changes\":[],\"age\":[]}\n",
    "\n",
    "possible_age_diff_values = list(regoup_df[\"age_abs_diff\"].drop_duplicates())\n",
    "for age_diff in possible_age_diff_values:\n",
    "    bar_dict[\"age\"].append(age_diff)\n",
    "    nb_age_p_changes = len(p_df[p_df['age_abs_diff']==age_diff])\n",
    "    bar_dict[\"pred changes\"].append(nb_age_p_changes)\n",
    "    nb_age_np_changes = len(np_df[np_df['age_abs_diff']==age_diff])\n",
    "    bar_dict[\"no pred changes\"].append(nb_age_np_changes)\n",
    "    print(f\"Age diff of {age_diff}: {nb_age_p_changes} pred changes and {nb_age_np_changes} no pred changes: {ratio(nb_age_p_changes,nb_age_np_changes)}% ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_df = pd.DataFrame(bar_dict)\n",
    "bar_df[\"total age cases\"] = bar_df[\"pred changes\"] + bar_df[\"no pred changes\"]\n",
    "bar_df[\"pred changes norm\"] = bar_df[\"pred changes\"]/bar_df[\"total age cases\"]\n",
    "bar_df[\"no pred changes norm\"] = bar_df[\"no pred changes\"]/bar_df[\"total age cases\"]\n",
    "bar_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_df.hvplot(\n",
    "    x=\"age\", \n",
    "    xlabel=\"ages\",\n",
    "    y=[\"pred changes norm\",\"no pred changes norm\"],\n",
    "    ylabel=\"amount of cases (normalised)\",\n",
    "    kind='bar', \n",
    "    #by=\"ages\",\n",
    "    #color=[pred_color_dict[name] for name in group_order],\n",
    "    title=\"normalised age change impact on prediction\",\n",
    "    width=1100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The amount of pred changes relative to the no pred changes seems to be indentical or to slightly increase the more age difference there is but this can be due to the fewer cases at larger differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [0, 0, 1, 1] # sex change (0:no,1:yes)\n",
    "y = [0, 1, 0, 1] # pred chage (0:no,1:yes)\n",
    "n = [nb_nsnp, nb_nsp, nb_snp, nb_sp] # dot text\n",
    "s = n # dot size\n",
    "c=[sex_pred_color_dict[name] for name in [nsnp_name,nsp_name,snp_name,sp_name]] # dot color\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(5)\n",
    "\n",
    "ax.scatter(x, y,s=s,c=c)\n",
    "ax.set(xlim=(-0.5, 1.5),ylim=(-0.5, 1.5))\n",
    "\n",
    "ax.set_title(\"sex change impact on prediction\")\n",
    "ax.set_xlabel(\"sex change\")\n",
    "ax.set_xticks([0,1], labels=[\"no\",\"yes\"], minor=False)\n",
    "ax.set_ylabel(\"pred change\")\n",
    "ax.set_yticks([0,1], labels=[\"no\",\"yes\"], minor=False)\n",
    "\n",
    "for i, txt in enumerate(n):\n",
    "    ax.annotate(txt, (x[i], y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sex change (noise of the sex data) isn't clearly responsible for a change of prediction capability since only ~150 cases were measured in this case. \\\n",
    "In contrast, ~550 cases were observed were the predition was changed even without changes to the sex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"% of prediction change when no sex change: {ratio(nb_nsp,nb_sp)}%\")\n",
    "print(f\"% of prediction change when sex change: {ratio(nb_nsnp,nb_snp)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The % of prediction change when with or without sex change are similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at only the cases where the model is wrong but confident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expl_miss_but_conf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_miss_but_conf = len(expl_miss_but_conf_df)\n",
    "print(\"Average age change while 'miss but confident':\",sum(expl_miss_but_conf_df[\"age_abs_diff\"])/nb_miss_but_conf)\n",
    "print(f\"While 'miss but confident' age was changed {len(expl_miss_but_conf_df[expl_miss_but_conf_df['age_abs_diff'] != 0])} times on {nb_miss_but_conf}\")\n",
    "print(f\"While 'miss but confident' sex was changed {len(expl_miss_but_conf_df[expl_miss_but_conf_df['sex_change']])} times on {nb_miss_but_conf}\")\n",
    "print(f\"While 'miss but confident' both were changed {len(expl_miss_but_conf_df[expl_miss_but_conf_df['sex_change'] & expl_miss_but_conf_df['age_abs_diff'] != 0])} times on {nb_miss_but_conf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UooALVf_2hvg"
   },
   "source": [
    "#  6 - Explainability and LLMs\n",
    "\n",
    "### Configuration LM server\n",
    "We assume that one will launch LM Studio and launch its server.\n",
    "\n",
    "Model used is same as in practice: Llama-3.2.3b-instruct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"llama-3.2-3b-instruct\", #not necessary as connected directly to localhost LM server\n",
    "    base_url=\"http://localhost:1234/v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specification for the model\n",
    "\n",
    "Here I specify to the system the context of the request, it tries to be an explainer for the results of LIME method and interprets it in human-readable text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_method=\"LIME\"\n",
    "context_system= (\"You're an interpreter of a explainability method for a classifier.\"\n",
    "                \" The method used is \" + explain_method + \".\"\n",
    "                \" You need to explain in a simple way anyone can understand what the values of the parameters from the explanation actually mean for the classifier.\")\n",
    "\n",
    "messages = [ \n",
    "    {\"role\": \"system\", \"content\": context_system}\n",
    "]\n",
    "\n",
    "def get_response(message, messages):\n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "    completion = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        temperature=0.4,\n",
    "        model=\"model\"\n",
    "    )\n",
    "    response = completion.choices[0].message.content\n",
    "    messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "    return response, messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of the example, we use an instance that was highly confident on a wrong prediction to learn why it mispredicted. We look at the first instance previously analyzed on [5. Explainability](#5---explainability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_id = miss_but_confident_detail[0]\n",
    "\n",
    "explainers = TabularExplainer(\n",
    "    explainers=['lime', 'mace'],                       # The explainers to apply\n",
    "    mode=\"classification\",                             # The task type\n",
    "    data=transformer.invert(X_train_scaled_p),         # The data for initializing the explainers\n",
    "    model=private_classifier,                          # The ML model to explain\n",
    "    preprocess=lambda z: transformer.transform(z),     # Converts raw features into the model inputs\n",
    "    params={\n",
    "        \"mace\": {\"ignored_features\": [\"Age\", \"Sex\"]}# params which cannot change when creating counterfactuals\n",
    "    }\n",
    ")\n",
    "\n",
    "test_instances = transformer.invert(X_test_scaled_p)[example_id:example_id+1]\n",
    "local_explanations = explainers.explain(X=test_instances)\n",
    "\n",
    "true_label, pred_label = Y_test_p.values[example_id], Y_pred_private_fair[example_id]\n",
    "probs = private_classifier.predict_proba(X_test_scaled_p[example_id:example_id+1])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We formulate the request here. One would give the pairs of features-importance scores but also the true label and prediction label to get an answer from the language through the server.\n",
    "\n",
    "Unfortunately, it takes a few minutes to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the features and their score with LIME method:\n",
      "\n",
      "Name of feature: capital-gain, Importance score: 0.5226965078276797\n",
      "Name of feature: education_Preschool, Importance score: -0.36581589319378627\n",
      "Name of feature: occupation_Priv-house-serv, Importance score: -0.3354421907663196\n",
      "Name of feature: workclass_Without-pay, Importance score: -0.2924377615117225\n",
      "Name of feature: occupation_Armed-Forces, Importance score: -0.2740265598030081\n",
      "Name of feature: marital-status_Married-AF-spouse, Importance score: 0.25956166029686994\n",
      "Name of feature: native-country_Hong, Importance score: 0.22241719269040844\n",
      "Name of feature: native-country_Cambodia, Importance score: 0.21128383610602153\n",
      "Name of feature: native-country_Columbia, Importance score: -0.20444894299948574\n",
      "Name of feature: native-country_Honduras, Importance score: -0.06931146789191218\n",
      "\n",
      "For the example 246, true label was True and prediction label was False. Can you explain to me what features were the most influential to get a bad prediction ? \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Let's analyze the LIME results for example 246.\n",
      "\n",
      "Since the true label was \"True\" but the predicted label was \"False\", we're looking for features that likely contributed to the incorrect prediction. The top 5 most negative importance scores, which indicate a decrease in probability of the positive class (in this case, being married), are:\n",
      "\n",
      "1. **education_Preschool**: -0.36581589319378627\n",
      "\t* This feature represents education level as preschool. Individuals with lower education levels might be less likely to be married or have a stable family life.\n",
      "2. **occupation_Priv-house-serv**: -0.3354421907663196\n",
      "\t* This feature categorizes individuals into private household servants, which is an occupation often associated with limited social mobility and potentially unstable living arrangements.\n",
      "3. **workclass_Without-pay**: -0.2924377615117225\n",
      "\t* This feature represents individuals without a regular pay, indicating they might be part of the informal economy or have limited financial security.\n",
      "4. **occupation_Armed-Forces**: -0.2740265598030081\n",
      "\t* Similar to the previous point, this feature indicates military personnel, who may have limited access to stable employment and family life.\n",
      "5. **native-country_Columbia**: -0.20444894299948574\n",
      "\t* This feature represents individuals from Colombia, which might be associated with cultural or socio-economic factors that influence marriage rates.\n",
      "\n",
      "These features collectively suggest that example 246 was mispredicted because:\n",
      "\n",
      "* They have a relatively low education level (preschool), which could indicate limited social capital and stability.\n",
      "* They work as private household servants, which is an occupation often associated with instability and limited access to resources.\n",
      "* They don't have regular pay, indicating they might be part of the informal economy or have limited financial security.\n",
      "* They are military personnel, who may have limited access to stable employment and family life.\n",
      "* They are from Colombia, a country with potentially different cultural or socio-economic factors that influence marriage rates.\n",
      "\n",
      "The model may have misinterpreted these features, leading to a prediction error. However, it's essential to note that LIME explanations are local and might not generalize well to other examples in the dataset.\n",
      "\n",
      "Keep in mind that this is just one possible interpretation, and there could be other factors at play. The results should be considered as a starting point for further investigation and validation of the model's performance.\n"
     ]
    }
   ],
   "source": [
    "lime_info_dict = local_explanations[\"lime\"].get_explanations()[0]\n",
    "feat_score_pairs = list(zip(lime_info_dict['features'],lime_info_dict['scores']))\n",
    "\n",
    "prefix = \"Here are the features and their score with \" + explain_method + \" method:\\n\\n\"\n",
    "for i in range (len(feat_score_pairs)):\n",
    "    prefix += \"Name of feature: \" + str(feat_score_pairs[i][0]) + \", Importance score: \" + str(feat_score_pairs[i][1]) + \"\\n\"\n",
    "\n",
    "suffix = f\"\\nFor the example {example_id}, true label was {true_label} and prediction label was {pred_label}. Can you explain to me what features were the most influential to get a bad prediction ?\"\n",
    "\n",
    "message = prefix + suffix\n",
    "print(message,\"\\n\")\n",
    "print(\"--------------------------------------------------------------------------------\")\n",
    "response, messages = get_response(message, messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3hJqqAY2hvg"
   },
   "source": [
    "# 7 - Free Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Last execution of this notebook:\",datetime.utcnow()+timedelta(hours=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
